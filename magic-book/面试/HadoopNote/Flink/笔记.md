## 知识点-flink基本介绍（了解）

### 1.目标

- 大数据发展历史
- Flink性能比较
- Flink在阿里的现状

### 2.讲解

​		磁盘平均存取时间= 寻道时间+旋转延迟时间（磁头定位到扇区时间）+数据传输时间

​		内存的访问速度比硬盘快，但是到底快多少？通常的说法是：内存访问速度是纳秒级（10的-9次方），硬盘的访问速度是微秒级（10的-3次方）。找到一个稍微科学点的测试数据，如下图

![1566778446227](./Flink/磁盘内存比较.png)

1.顺序访问：这种情况下，内存访问速度仅仅是硬盘访问速度的6~7倍（358.2M / 53.2M = 6.7）

2.随机访问：这种情况下，内存访问速度就要比硬盘访问速度快上10万倍以上 （36.7M / 316 = 113,924）

### 3.小结

了解大数据计算引擎的都包括哪些开源项目，国外社区认为的4代计算引擎他们各自的功能与比较，其中对spark与hadoopMR及spark和flink的比较；

了解flink作为分布式计算引擎，他支持哪些应用及应用场景？

数据流上的有状态的计算是什么？数据流和有状态的计算？其他的一些特点？

flink与hadoopMR及spark通过kmeans聚类算法迭代的性能比较？

大数据引擎的发展历史？（了解）

flink在阿里的现状？（了解）

​	

## 知识点-flink的集群安装（会用）

### 1.目标

- 伪分布式安装
- StandAlone模式安装
- StandAlone的高可用模式
- Yarn模式
- Yarn-Session

### 2.步骤

1.安装本地模式

​		*1.先将flink-1.7.2.jar上传到服务器的  /opt/目录下*

​		*2.解压缩jar包到 /usr/local 目录下*

​		*3.修改配置文件 /usr/local/flink-1.7.2/conf/flink-conf.yaml*

​		*4.启动flink  /usr/local/flink-1.7.2/bin/start-cluster.sh*

​		*5.查看进行 jps*

```shell
[root@itheimasinglenode ~]# start-cluster.sh 
Starting cluster.
Starting standalonesession daemon on host itheimasinglenode.
Starting taskexecutor daemon on host itheimasinglenode. 
[root@itheimasinglenode ~]# jps
2513 StandaloneSessionClusterEntrypoint
3086 Jps
2975 TaskManagerRunner
```

2.安装standalone模式

​		*与本地模式最大区别*

​		*1.需要将配置文件拷贝到别的节点*

​		*2.需要配置slave配置，将worker添加到slave*

3.安装standalone的HA高可用模式

​		*与standalone模式最大区别*

​		*1.需要修改flink-conf.yaml文件的*

![1566457942749](./Flink/1566457942749.png)

​		*2.需要修改master配置文件*

![1566458113277](./Flink/1566458113277.png)

​		*将所有的 jobmanager 的 hostname 或者 IP:8081 一行一个*

4.安装Yarn模式

​		*配置yarn-site.xml 新增以下内容*，并且通过 stop-yarn.sh start-yarn.sh重启yarn

```<property>
<name>yarn.nodemanager.vmem-check-enabled</name>
<value>false</value>
</property>
```

5.提交yarn-session 尝试启动任务

​		*1.会话模式*

```bin/yarn-session.sh -n 2 -tm 800 -s 1 -d
bin/yarn-session.sh -n 2 -tm 800 -s 1 -d

# -n 表示申请2个容器，
# -s 表示每个容器启动多少个slot
# -tm 表示每个TaskManager申请800M内存
# -d 表示以后台程序方式运行

yarn-session.sh脚本可以携带的参数:
   Required
     -n,--container <arg>               分配多少个yarn容器 (=taskmanager的数量)  
   Optional
     -D <arg>                        动态属性
     -d,--detached                    独立运行 （以分离模式运行作业）
     -id,--applicationId <arg>            YARN集群上的任务id，附着到一个后台运行的yarn session中
     -j,--jar <arg>                      Path to Flink jar file
     -jm,--jobManagerMemory <arg>     JobManager的内存 [in MB] 
     -m,--jobmanager <host:port>        指定需要连接的jobmanager(主节点)地址  
                                    使用这个参数可以指定一个不同于配置文件中的jobmanager  
     -n,--container <arg>               分配多少个yarn容器 (=taskmanager的数量) 
     -nm,--name <arg>                 在YARN上为一个自定义的应用设置一个名字
     -q,--query                        显示yarn中可用的资源 (内存, cpu核数) 
     -qu,--queue <arg>                 指定YARN队列
     -s,--slots <arg>                   每个TaskManager使用的slots数量
     -st,--streaming                   在流模式下启动Flink
     -tm,--taskManagerMemory <arg>    每个TaskManager的内存 [in MB] 
     -z,--zookeeperNamespace <arg>     针对HA模式在zookeeper上创建NameSpace
```

​		*2.分离模式*

```
bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar  

# -m  jobmanager的地址
# -yn 表示TaskManager的个数 
```



### 3.小结



## 知识点-flink架构介绍（掌握）

### 1.目标

- Flink组件栈
- 编程模型
- 程序结构
- 任务调度与执行
- Flink的应用场景

### 2.讲解

### 3.小结



## 实操-Batch（Dataset）开发（会用）

### 1.目标

​		为了开发的Flink DataSet程序，先从Flink程序WordCount开始，并逐步添加Transformation。了解其高级特性，如何使用Batch 操作。

### 2.路径

​		WordCount案例项目实战

​		Dataset 转换操作

​		data source （比如读Compressed Files）

​		data sink

​		迭代操作（Iteration Operators）

​		Debugging（本地执行环境和数据集合source）

​		Semantic Annotations

​		广播变量

​		分布式缓存

​		将参数传递给函数

### 3.讲解

​		WordCount案例项目实战，分为Java和scala两个版本

```java
public class WordCountExample {
    public static void main(String[] args) throws Exception {
        final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();

        DataSet<String> text = env.fromElements(
            "Who's there?",
            "I think I hear them. Stand, ho! Who's there?");

        DataSet<Tuple2<String, Integer>> wordCounts = text
            .flatMap(new LineSplitter())
            .groupBy(0)
            .sum(1);

        wordCounts.print();
    }

    public static class LineSplitter implements FlatMapFunction<String, Tuple2<String, Integer>> {
        @Override
        public void flatMap(String line, Collector<Tuple2<String, Integer>> out) {
            for (String word : line.split(" ")) {
                out.collect(new Tuple2<String, Integer>(word, 1));
            }
        }
    }
}
```

```scala
import org.apache.flink.api.scala._

object WordCount {
  def main(args: Array[String]) {

    val env = ExecutionEnvironment.getExecutionEnvironment
    val text = env.fromElements(
      "Who's there?",
      "I think I hear them. Stand, ho! Who's there?")

    val counts = text.flatMap { _.toLowerCase.split("\\W+") filter { _.nonEmpty } }
      .map { (_, 1) }
      .groupBy(0)
      .sum(1)

    counts.print()
  }
}
```

DataSet 转换操作

|   transformation    | 描述                                                         |
| :-----------------: | ------------------------------------------------------------ |
|         map         | data.map { x => x.toInt }                                    |
|       flatMap       | data.flatMap { str => str.split(" ") }                       |
|    mapPartition     | data.mapPartition { in => in map { (_, 1) } }                |
|       filter        | data.filter { _ > 1000 }                                     |
|       reduce        | data.reduce { _ + _ }                                        |
|     reduceGroup     | data.reduceGroup { elements => elements.sum }                |
|      aggragate      | aggregates一组值为一个值. Aggregation functions 被认为是内建 reduce 函数。aggregate 被应用到全数据集和一组数据集上。val input: DataSet[(Int, String, Double)] = // [...] val output: DataSet[(Int, String, Double)] = input.aggregate(SUM, 0).aggregate(MIN, 2) |
|      distinct       | data.distinct()                                              |
|        join         | val result = input1.join(input2).where(0).equalTo(1)         |
|      outerJoin      | val joined = left.leftOuterJoin(right).where(0).equalTo(1) {  (left, right) =>  val a = if (left == null) "none" else left._1      (a, right)   } |
|       coGroup       | data1.coGroup(data2).where(0).equalTo(1) ，注重group 和join类似 |
|        cross        | val data1: DataSet[Int] = // [...]                                                                                               val data2: DataSet[String] = // [...]                                                                                       val result: DataSet[(Int, String)] = data1.cross(data2) |
|        union        | data.union(data2)                                            |
|      rebalance      | val data1: DataSet[Int] = // [...]                                                                                             val result: DataSet[(Int, String)] = data1.rebalance().map(...) |
|   hash-Partition    | val in: DataSet[(Int, String)] = // [...]                                                                                     val result = in.partitionByHash(0).mapPartition { ... } |
|   range-Partition   | val in: DataSet[(Int, String)] = // [...]                                                                                     val result = in.partitionByRange(0).mapPartition { ... } |
| custom Partitioning | val in: DataSet[(Int, String)] = // [...]                                                                                     val result = in.partitionCustom(partitioner, key).mapPartition { ... } |
|   sort Partition    | val in: DataSet[(Int, String)] = // [...] val result = in.sortPartition(1, Order.ASCENDING).mapPartition { ... } |
|       first-n       | val in: DataSet[(Int, String)] = // [...] // regular data set                                                   val result1 = in.first(3) // grouped data set                                                                        val result2 = in.groupBy(0).first(3) // grouped-sorted data set                                         val result3 = in.groupBy(0).sortGroup(1, Order.ASCENDING).first(3) |
|                     |                                                              |

data source （比如读Compressed Files）

```SCALA
val env  = ExecutionEnvironment.getExecutionEnvironment

// read text file from local files system
val localLines = env.readTextFile("file:///path/to/my/textfile")

// read text file from a HDFS running at nnHost:nnPort
val hdfsLines = env.readTextFile("hdfs://nnHost:nnPort/path/to/my/textfile")

// read a CSV file with three fields
val csvInput = env.readCsvFile[(Int, String, Double)]("hdfs:///the/CSV/file")

// read a CSV file with five fields, taking only two of them
val csvInput = env.readCsvFile[(String, Double)](
  "hdfs:///the/CSV/file",
  includedFields = Array(0, 3)) // take the first and the fourth field

// CSV input can also be used with Case Classes
case class MyCaseClass(str: String, dbl: Double)
val csvInput = env.readCsvFile[MyCaseClass](
  "hdfs:///the/CSV/file",
  includedFields = Array(0, 3)) // take the first and the fourth field

// read a CSV file with three fields into a POJO (Person) with corresponding fields
val csvInput = env.readCsvFile[Person](
  "hdfs:///the/CSV/file",
  pojoFields = Array("name", "age", "zipcode"))

// create a set from some given elements
val values = env.fromElements("Foo", "bar", "foobar", "fubar")

// generate a number sequence
val numbers = env.generateSequence(1, 10000000)

// read a file from the specified path of type SequenceFileInputFormat
val tuples = env.createInput(HadoopInputs.readSequenceFile(classOf[IntWritable], classOf[Text],
 "hdfs://nnHost:nnPort/path/to/file"))
```

读compresseed Files

| 压缩方法    | 文件扩展名     | 可并行性 |
| ----------- | -------------- | -------- |
| **DEFLATE** | .deflate       | no       |
| **GZip**    | `.gz`, `.gzip` | no       |
| **Bzip2**   | .bz2           | no       |
| **XZ**      | .xz            | no       |

Data Sink

标准得data sink 方法：

```JAVA
/ text data
DataSet<String> textData = // [...]

// write DataSet to a file on the local file system
textData.writeAsText("file:///my/result/on/localFS");

// write DataSet to a file on a HDFS with a namenode running at nnHost:nnPort
textData.writeAsText("hdfs://nnHost:nnPort/my/result/on/localFS");

// write DataSet to a file and overwrite the file if it exists
textData.writeAsText("file:///my/result/on/localFS", WriteMode.OVERWRITE);

// tuples as lines with pipe as the separator "a|b|c"
DataSet<Tuple3<String, Integer, Double>> values = // [...]
values.writeAsCsv("file:///path/to/the/result/file", "\n", "|");

// this writes tuples in the text formatting "(a, b, c)", rather than as CSV lines
values.writeAsText("file:///path/to/the/result/file");

// this writes values as strings using a user-defined TextFormatter object
values.writeAsFormattedText("file:///path/to/the/result/file",
    new TextFormatter<Tuple2<Integer, Integer>>() {
        public String format (Tuple2<Integer, Integer> value) {
            return value.f1 + " - " + value.f0;
        }
    });
用自定义得数据格式：

DataSet<Tuple3<String, Integer, Double>> myResult = [...]

// write Tuple DataSet to a relational database
myResult.output(
    // build and configure OutputFormat
    JDBCOutputFormat.buildJDBCOutputFormat()
                    .setDrivername("org.apache.derby.jdbc.EmbeddedDriver")
                    .setDBUrl("jdbc:derby:memory:persons")
                    .setQuery("insert into persons (name, age, height) values (?,?,?)")
                    .finish()
    );
```

```scala
// text data
val textData: DataSet[String] = // [...]

// write DataSet to a file on the local file system
textData.writeAsText("file:///my/result/on/localFS")

// write DataSet to a file on a HDFS with a namenode running at nnHost:nnPort
textData.writeAsText("hdfs://nnHost:nnPort/my/result/on/localFS")

// write DataSet to a file and overwrite the file if it exists
textData.writeAsText("file:///my/result/on/localFS", WriteMode.OVERWRITE)

// tuples as lines with pipe as the separator "a|b|c"
val values: DataSet[(String, Int, Double)] = // [...]
values.writeAsCsv("file:///path/to/the/result/file", "\n", "|")

// this writes tuples in the text formatting "(a, b, c)", rather than as CSV lines
values.writeAsText("file:///path/to/the/result/file")

// this writes values as strings using a user-defined formatting
values map { tuple => tuple._1 + " - " + tuple._2 }
  .writeAsText("file:///path/to/the/result/file")
```

迭代操作 Interator operators

```java
final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();

// Create initial IterativeDataSet
IterativeDataSet<Integer> initial = env.fromElements(0).iterate(10000);

DataSet<Integer> iteration = initial.map(new MapFunction<Integer, Integer>() {
    @Override
    public Integer map(Integer i) throws Exception {
        double x = Math.random();
        double y = Math.random();

        return i + ((x * x + y * y < 1) ? 1 : 0);
    }
});

// Iteratively transform the IterativeDataSet
DataSet<Integer> count = initial.closeWith(iteration);

count.map(new MapFunction<Integer, Double>() {
    @Override
    public Double map(Integer count) throws Exception {
        return count / (double) 10000 * 4;
    }
}).print();

env.execute("Iterative Pi Example");
```

```scala
val env = ExecutionEnvironment.getExecutionEnvironment()

// Create initial DataSet
val initial = env.fromElements(0)

val count = initial.iterate(10000) { iterationInput: DataSet[Int] =>
  val result = iterationInput.map { i =>
    val x = Math.random()
    val y = Math.random()
    i + (if (x * x + y * y < 1) 1 else 0)
  }
  result
}

val result = count map { c => c / 10000.0 * 4 }

result.print()

env.execute("Iterative Pi Example")
```

调试

在线上分布式环境运行之前，要确保我们得程序基本逻辑没有问题，这里部署flink程序提供一些帮助：

​	本地运行环境

```JAVA
final ExecutionEnvironment env = ExecutionEnvironment.createLocalEnvironment();
DataSet<String> lines = env.readTextFile(pathToTextFile);
// build your program
env.execute();
```

```scala
val env = ExecutionEnvironment.createLocalEnvironment()
val lines = env.readTextFile(pathToTextFile)
// build your program
env.execute()
```

​	source的集合数据

```JAVA
final ExecutionEnvironment env = ExecutionEnvironment.createLocalEnvironment();

// Create a DataSet from a list of elements
DataSet<Integer> myInts = env.fromElements(1, 2, 3, 4, 5);

// Create a DataSet from any Java collection
List<Tuple2<String, Integer>> data = ...
DataSet<Tuple2<String, Integer>> myTuples = env.fromCollection(data);

// Create a DataSet from an Iterator
Iterator<Long> longIt = ...
DataSet<Long> myLongs = env.fromCollection(longIt, Long.class);
```

```scala
val env = ExecutionEnvironment.createLocalEnvironment()

// Create a DataSet from a list of elements
val myInts = env.fromElements(1, 2, 3, 4, 5)

// Create a DataSet from any Collection
val data: Seq[(String, Int)] = ...
val myTuples = env.fromCollection(data)

// Create a DataSet from an Iterator
val longIt: Iterator[Long] = ...
val myLongs = env.fromCollection(longIt)
```

语义标注

```java
@ForwardedFields("f0->f2")
public class MyMap implements
              MapFunction<Tuple2<Integer, Integer>, Tuple3<String, Integer, Integer>> {
  @Override
  public Tuple3<String, Integer, Integer> map(Tuple2<Integer, Integer> val) {
    return new Tuple3<String, Integer, Integer>("foo", val.f1 / 2, val.f0);
  }
}
```

```scala
@ForwardedFields("_1->_3")
class MyMap extends MapFunction[(Int, Int), (String, Int, Int)]{
   def map(value: (Int, Int)): (String, Int, Int) = {
    return ("foo", value._2 / 2, value._1)
  }
}
```

​	读字段

```JAVA
@ReadFields("f0; f3") // f0 and f3 are read and evaluated by the function.
public class MyMap implements
              MapFunction<Tuple4<Integer, Integer, Integer, Integer>,
                          Tuple2<Integer, Integer>> {
  @Override
  public Tuple2<Integer, Integer> map(Tuple4<Integer, Integer, Integer, Integer> val) {
    if(val.f0 == 42) {
      return new Tuple2<Integer, Integer>(val.f0, val.f1);
    } else {
      return new Tuple2<Integer, Integer>(val.f3+10, val.f1);
    }
  }
}
```

```SCALA
@ReadFields("_1; _4") // _1 and _4 are read and evaluated by the function.
class MyMap extends MapFunction[(Int, Int, Int, Int), (Int, Int)]{
   def map(value: (Int, Int, Int, Int)): (Int, Int) = {
    if (value._1 == 42) {
      return (value._1, value._2)
    } else {
      return (value._4 + 10, value._2)
    }
  }
}
```

广播变量

广播变量传递:广播变量通过broadcastset (DataSet, String)和按withBroadcastSet(DataSet, String)。

广播变量访问:通过目标操作符上的getRuntimeContext(). getbroadcastvariable (String)访问。

```java
// 1. The DataSet to be broadcast
DataSet<Integer> toBroadcast = env.fromElements(1, 2, 3);

DataSet<String> data = env.fromElements("a", "b");

data.map(new RichMapFunction<String, String>() {
    @Override
    public void open(Configuration parameters) throws Exception {
      // 3. Access the broadcast DataSet as a Collection
      Collection<Integer> broadcastSet = getRuntimeContext().getBroadcastVariable("broadcastSetName");
    }

    @Override
    public String map(String value) throws Exception {
        ...
    }
}).withBroadcastSet(toBroadcast, "broadcastSetName"); // 2. Broadcast the DataSet
```

```scala
// 1. The DataSet to be broadcast
val toBroadcast = env.fromElements(1, 2, 3)

val data = env.fromElements("a", "b")

data.map(new RichMapFunction[String, String]() {
    var broadcastSet: Traversable[String] = null

    override def open(config: Configuration): Unit = {
      // 3. Access the broadcast DataSet as a Collection
      broadcastSet = getRuntimeContext().getBroadcastVariable[String]("broadcastSetName").asScala
    }

    def map(in: String): String = {
        ...
    }
}).withBroadcastSet(toBroadcast, "broadcastSetName") // 2. Broadcast the DataSet
```

​		在注册和访问广播数据集时，确保名称(上例中的broadcastSetName)匹配。要获得完整的示例程序，请查看KMeans算法。

分布式缓存

​		它包含静态外部数据，如字典（dictionaries）或机器学习的回归模型。缓存的工作原理如下：程序在其执行环境中以特定名称注册本地或远程文件系统(如HDFS或S3)的文件或目录作为缓存文件。当程序执行时，Flink自动将文件或目录复制到所有工作人员的本地文件系统。用户函数可以查找指定名称下的文件或目录，并从工作人员的本地文件系统中访问它。

```JAVA
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();

// register a file from HDFS
env.registerCachedFile("hdfs:///path/to/your/file", "hdfsFile")

// register a local executable file (script, executable, ...)
env.registerCachedFile("file:///path/to/exec/file", "localExecFile", true)

// define your program and execute
...
DataSet<String> input = ...
DataSet<Integer> result = input.map(new MyMapper());
...
env.execute();
```

```SCALA
val env = ExecutionEnvironment.getExecutionEnvironment

// register a file from HDFS
env.registerCachedFile("hdfs:///path/to/your/file", "hdfsFile")

// register a local executable file (script, executable, ...)
env.registerCachedFile("file:///path/to/exec/file", "localExecFile", true)

// define your program and execute
...
val input: DataSet[String] = ...
val result: DataSet[Integer] = input.map(new MyMapper())
...
env.execute()
```

​		在用户函数中访问缓存的文件(这里是一个map函数)。函数必须扩展RichFunction类，因为它需要访问RuntimeContext。

```JAVA
// extend a RichFunction to have access to the RuntimeContext
public final class MyMapper extends RichMapFunction<String, Integer> {

    @Override
    public void open(Configuration config) {

      // access cached file via RuntimeContext and DistributedCache
      File myFile = getRuntimeContext().getDistributedCache().getFile("hdfsFile");
      // read the file (or navigate the directory)
      ...
    }

    @Override
    public Integer map(String value) throws Exception {
      // use content of cached file
      ...
    }
}
```

```SCALA
// extend a RichFunction to have access to the RuntimeContext
class MyMapper extends RichMapFunction[String, Int] {

  override def open(config: Configuration): Unit = {

    // access cached file via RuntimeContext and DistributedCache
    val myFile: File = getRuntimeContext.getDistributedCache.getFile("hdfsFile")
    // read the file (or navigate the directory)
    ...
  }

  override def map(value: String): Int = {
    // use content of cached file
    ...
  }
}
```

将参数传递给函数

​		可以使用构造函数或withParameters(配置)方法将参数传递给函数。参数被序列化为函数对象的一部分，并传送到所有并行任务实例。

via Constructor：

```JAVA
DataSet<Integer> toFilter = env.fromElements(1, 2, 3);

toFilter.filter(new MyFilter(2));

private static class MyFilter implements FilterFunction<Integer> {

  private final int limit;

  public MyFilter(int limit) {
    this.limit = limit;
  }

  @Override
  public boolean filter(Integer value) throws Exception {
    return value > limit;
  }
}
```

```SCALA
val toFilter = env.fromElements(1, 2, 3)

toFilter.filter(new MyFilter(2))

class MyFilter(limit: Int) extends FilterFunction[Int] {
  override def filter(value: Int): Boolean = {
    value > limit
  }
}
```

via withParameters(config)

```java
DataSet<Integer> toFilter = env.fromElements(1, 2, 3);

Configuration config = new Configuration();
config.setInteger("limit", 2);

toFilter.filter(new RichFilterFunction<Integer>() {
    private int limit;

    @Override
    public void open(Configuration parameters) throws Exception {
      limit = parameters.getInteger("limit", 0);
    }

    @Override
    public boolean filter(Integer value) throws Exception {
      return value > limit;
    }
}).withParameters(config);
```

```scala
val toFilter = env.fromElements(1, 2, 3)

val c = new Configuration()
c.setInteger("limit", 2)

toFilter.filter(new RichFilterFunction[Int]() {
    var limit = 0

    override def open(config: Configuration): Unit = {
      limit = config.getInteger("limit", 0)
    }

    def filter(in: Int): Boolean = {
        in > limit
    }
}).withParameters(c)
```

通过ExecutionConfig全局执行

​		Flink还允许将自定义配置值传递给环境的ExecutionConfig接口。由于执行配置在所有(富)用户函数中都是可访问的，所以自定义配置将在所有函数中全局可用。
设置自定义全局配置

```java
Configuration conf = new Configuration();
conf.setString("mykey","myvalue");
final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
env.getConfig().setGlobalJobParameters(conf);
```

```scala
val env = ExecutionEnvironment.getExecutionEnvironment
val conf = new Configuration()
conf.setString("mykey", "myvalue")
env.getConfig.setGlobalJobParameters(conf)
```

​		请注意，您还可以传递扩展ExecutionConfig的自定义类。类作为执行配置的全局作业参数。该接口允许实现Map<String, String> toMap()方法，该方法将依次显示来自web前端配置的值。

​		从全局配置访问值
全局作业参数中的对象可以在系统中的许多地方访问。所有实现RichFunction接口的用户函数都可以通过运行时上下文进行访问。

```java
public static final class Tokenizer extends RichFlatMapFunction<String, Tuple2<String, Integer>> {

    private String mykey;
    @Override
    public void open(Configuration parameters) throws Exception {
      super.open(parameters);
      ExecutionConfig.GlobalJobParameters globalParams = getRuntimeContext().getExecutionConfig().getGlobalJobParameters();
      Configuration globConf = (Configuration) globalParams;
      mykey = globConf.getString("mykey", null);
    }
    // ... more here ...
```

```java
Configuration conf = new Configuration();
conf.setString("mykey","myvalue");
final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
env.getConfig().setGlobalJobParameters(conf);
```

```scala
val env = ExecutionEnvironment.getExecutionEnvironment
val conf = new Configuration()
conf.setString("mykey", "myvalue")
env.getConfig.setGlobalJobParameters(conf)
```



### 4.小结



## 实操-Streaming开发（会用）

### 1.目标

​		flink streaming 操作是在数据流上产生的操作， (e.g., filtering, updating state, defining windows, aggregating)，了解其高级特性，如何使用Streaming操作。

### 2.路径

​		Wordcount 案例项目实战

​		数据源

​		数据流转换

​		数据Sink

​		数据迭代

​		执行参数

​				容错机制

​				控制延迟

### 3.讲解

​		wordcount案例项目实战

```JAVA
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.util.Collector;

public class WindowWordCount {

    public static void main(String[] args) throws Exception {

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<Tuple2<String, Integer>> dataStream = env
                .socketTextStream("localhost", 9999)
                .flatMap(new Splitter())
                .keyBy(0)
                .timeWindow(Time.seconds(5))
                .sum(1);

        dataStream.print();

        env.execute("Window WordCount");
    }

    public static class Splitter implements FlatMapFunction<String, Tuple2<String, Integer>> {
        @Override
        public void flatMap(String sentence, Collector<Tuple2<String, Integer>> out) throws Exception {
            for (String word: sentence.split(" ")) {
                out.collect(new Tuple2<String, Integer>(word, 1));
            }
        }
    }
}
```

```SCALA
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.windowing.time.Time

object WindowWordCount {
  def main(args: Array[String]) {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    val text = env.socketTextStream("localhost", 9999)

    val counts = text.flatMap { _.toLowerCase.split("\\W+") filter { _.nonEmpty } }
      .map { (_, 1) }
      .keyBy(0)
      .timeWindow(Time.seconds(5))
      .sum(1)
    counts.print()
    env.execute("Window Stream WordCount")
  }
}
```

```SHELL
nc -lk 9999
```

​	数据源（同Dataset）

```
有几个预定义的流资源可以从StreamExecutionEnvironment访问:
File-based:
- readTextFile(path)
- readFile(fileInputFormat, path)
- readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo)
Socket-based:
- socketTextStream 
Collection-based:
- fromCollection(Collection)
- fromCollection(Iterator, Class)
- fromElements(T ...)
- fromParallelCollection(SplittableIterator, Class)
- generateSequence(from, to)
自定义
- addSource
```

| -方法名                                                      | -含义                                                        |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| readTextFile(path)                                           | 逐行读取文本文件，即尊重TextInputFormat规范的文件，并将它们作为字符串返回。 |
| readFile(fileInputFormat, path)                              | 按照指定的文件输入格式读取(一次)文件。                       |
| readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) | 这是前两个方法在内部调用的方法。它根据给定的fileInputFormat读取路径中的文件。 |
| socketTextStream                                             | 从socket的读取数据。                                         |

​	数据转换 （具体参考 https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/index.html）

| **Transformation**                  | 用法描述                       |
| ----------------------------------- | ------------------------------ |
| **Map**<br/>DataStream → DataStream | 一个数据流转换成另外一个数据流 |

​	数据sink

```
- writeAsText() / TextOutputFormat
- writeAsCsv(...) / CsvOutputFormat
- print() / printToErr()
- writeUsingOutputFormat() / FileOutputFormat
- writeToSocket
- addSink  ---调用自定义接收器函数
```

​		为了可靠、准确地将流交付到文件系统，请使用flink-connector-filesystem。此外，通过. addsink(…)方法的自定义实现可以参与Flink的检查点，以获得精确的一次语义。

迭代Iterations

​		迭代流程序实现了一个step函数并将其嵌入到IterativeStream中。由于DataStream程序可能永远不会完成，所以没有最大迭代次数。相反，您需要指定流的哪一部分被反馈回迭代，以及哪一部分使用拆分转换或过滤器被转发到下游。这里，我们展示一个使用过滤器的例子。首先，我们定义一个IterativeStream

```
IterativeStream<Integer> iteration = input.iterate();
```

​		然后，我们指定将在循环中使用一系列转换执行的逻辑(这里是一个简单的映射转换)

```
DataStream<Integer> iterationBody = iteration.map(/* this is executed many times */);
```

​		要关闭迭代并定义迭代尾部，请调用IterativeStream的closeWith(feedbackStream)方法。给closeWith函数的数据流将反馈给迭代头。一种常见的模式是使用过滤器来分离返回的流的一部分和转发的流的一部分。例如，这些过滤器可以定义“终止”逻辑，其中允许元素向下传播而不是返回。

```
iteration.closeWith(iterationBody.filter(/* one part of the stream */));
DataStream<Integer> output = iterationBody.filter(/* some other part of the stream */);
```

​		例如，这里有一个程序，它不断地从一系列整数中减去1，直到它们达到零:

```JAVA
DataStream<Long> someIntegers = env.generateSequence(0, 1000);

IterativeStream<Long> iteration = someIntegers.iterate();

DataStream<Long> minusOne = iteration.map(new MapFunction<Long, Long>() {
  @Override
  public Long map(Long value) throws Exception {
    return value - 1 ;
  }
});

DataStream<Long> stillGreaterThanZero = minusOne.filter(new FilterFunction<Long>() {
  @Override
  public boolean filter(Long value) throws Exception {
    return (value > 0);
  }
});

iteration.closeWith(stillGreaterThanZero);

DataStream<Long> lessThanZero = minusOne.filter(new FilterFunction<Long>() {
  @Override
  public boolean filter(Long value) throws Exception {
    return (value <= 0);
  }
});
```

​	执行参数

​	StreamExecutionEnvironment包含ExecutionConfig，它允许为运行时设置特定于作业的配置值。
有关大多数参数的说明，请参阅执行配置。这些参数专门属于DataStream API:

- setAutoWatermarkInterval(long millionseconds):设置自动水印emission的时间间隔。可以使用长getAutoWatermarkInterval()获取当前值。

  容错机制

​		state和checkpointing描述如何启用和配置Flink的检查点机制。

​	控制延时

​		默认情况下，元素不会在网络上逐个传输(这会导致不必要的网络流量)，而是进行缓冲。缓冲区的大小(实际上是在机器之间传输的)可以在Flink配置文件中设置。虽然这种方法很适合优化吞吐量，但是当传入流不够快时，它可能会导致延迟问题。要控制吞吐量和延迟，可以在执行环境(或单个操作符)上使用env.setBufferTimeout(timeoutMillis)设置缓冲区填充的最大等待时间。在此之后，即使缓冲区没有满，也会自动发送缓冲区。此超时的默认值为100 ms。

```JAVA
LocalStreamEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();
env.setBufferTimeout(timeoutMillis);

env.generateSequence(1,10).map(new MyMapper()).setBufferTimeout(timeoutMillis);
```

```SCALA
val env: LocalStreamEnvironment = StreamExecutionEnvironment.createLocalEnvironment
env.setBufferTimeout(timeoutMillis)

env.generateSequence(1,10).map(myMap).setBufferTimeout(timeoutMillis)
```

​		为了最大限度地提高吞吐量，设置setBufferTimeout(-1)，它将删除超时，并且只有当缓冲区已满时才会刷新缓冲区。为了最小化延迟，将超时设置为接近0的值(例如5或10 ms)。应该避免缓冲区超时为0，因为这会导致严重的性能下降。



### 4.小结

flink数据流处理的算子和flink批处理有很多是相似的，可以参考flink batch转换算子，流处理有一些自己的转换操作算子，比如基于窗口处理的，迭代流处理等。
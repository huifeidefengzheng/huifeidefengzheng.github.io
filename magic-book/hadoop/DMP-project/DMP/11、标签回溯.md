#### 2.15.5.8：标签回溯

所谓的标签回溯就是：拿到衰减后的历史标签数据与当天生成的标签数据合并，然后进行统一用户识别以及标签聚合（之所以要在进行一遍同一用户识别和标签聚合，是因为很有可能当天标签数据与历史标签数据存在多行，并且标签重合的问题），最终数据落地

代码:

```scala
package merge

import agg.TagAgg
import attenu.TagAttenu
import graph.UserGraph
import org.apache.spark.graphx.VertexId
import org.apache.spark.rdd.RDD

/**
  * 当天标签数据与历史标签数据的合并
  */
object TagMerge {
  /**
    * 问题:
    * 1、只有当天的标签数据
    * 2、当天数据与历史数据合并问题:
    *   1、历史标签权重如果处理
    *   2、合并后出现一个用户多条记录的情况
    *   3、历史数据与当天数据中出现相同标签怎么处理
    *
    * 解决方案:
    *   1、与历史数据合并
    *   2、历史标签权重如果处理：使用牛顿冷却定律==> 当前温度 = 初始温度 * exp(-冷却系数 * 时间间隔)
    *      标签衰减公式 ==> 当前权重 = 历史权重 * 衰减系数(自定为0.8)
    *   3、用用户统一识别来解决历史数据与当天数据合并后出现一个用户出现多条记录的情况
    *   4、标签聚合
    * @param historyData
    * @param currentData
    */
  def merge(historyData:RDD[(String, (List[String], Map[String, Double]))],currentData:RDD[(String, (List[String], Map[String, Double]))]):RDD[(String, (List[String], Map[String, Double]))]={

    //1、历史数据标签衰减
    val historyTagRdd: RDD[(String, (List[String], Map[String, Double]))] = TagAttenu.attenu(historyData)
    //2、历史数据与当天数据的合并
    val tagsRdd: RDD[(String, (List[String], Map[String, Double]))] = historyTagRdd.union(currentData)
    //3、用户统一识别
    val graph: RDD[(VertexId, (VertexId, (List[String], Map[String, Double])))] = UserGraph.graph(tagsRdd)
    //4、标签聚合
    val resultTags: RDD[(VertexId, (List[String], Map[String, Double]))] = TagAgg.agg(graph)

    resultTags.map{
      case (aggid,(allUserIds,tags)) => (allUserIds.head,(allUserIds,tags))
    }
  }
}

```

#### 2.15.5.9：打标签、统一用户识别、标签聚合、标签衰减、标签回溯：全部代码

##### 2.15.5.9.1：APP添加代码

```scala
//TODO 10):数据标签化-衰减化-标签回溯
Merge_tags.process(sqlContext,sparkContext,kuduContext)
```

##### 2.15.5.9.2：Merge_tags代码

```scala
package pro

import java.sql.DriverManager

import `trait`.Process
import agg.TagAgg
import ch.hsr.geohash.GeoHash
import graph.UserGraph
import merge.TagMerge
import org.apache.commons.lang3.StringUtils
import org.apache.kudu.client.CreateTableOptions
import org.apache.kudu.spark.kudu.KuduContext
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.graphx.VertexId
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{Row, SaveMode, SparkSession}
import tag._
import utils.{ConfigUtils, DateUtils, KuduUtils}

/**
  * 数据标签化
  */
object TagProcess extends Process{

  //指定数据原表
  val SOURCE_TABLE = s"ODS_${DateUtils.getNow()}"
  //指定kudu集群的信息
  val options = Map[String,String](
    "kudu.master"->ConfigUtils.KUDU_MASTER,
    "kudu.table"->SOURCE_TABLE
  )

  //指定商圈表
  val BUSINESS_AREA_TABLE = s"business_area_${DateUtils.getNow()}"
  //指定kudu集群商圈信息
  val areaOptions = Map[String,String](
    "kudu.master"->ConfigUtils.KUDU_MASTER,
    "kudu.table"->BUSINESS_AREA_TABLE
  )
  //历史表
  val HISTORY_TABLE = s"TAG_${DateUtils.getYesterDay()}"

  val historyOptions = Map[String,String](
    "kudu.master"->ConfigUtils.KUDU_MASTER,
    "kudu.table"->HISTORY_TABLE
  )

  //标签存储表
  val SINK_TABLE = s"TAG_${DateUtils.getNow()}"
  /**
    * 逻辑处理部分，后期不同的操作写不同的逻辑
    *
    * 标签:
    *   app、设备标签(设备型号、设备类型01、设备类型02、运营商、联网方式)、地域标签(省份、城市)、广告位类型、
    *   关键字、渠道、性别、年龄
    * 用户标识
    */
  override def process(spark: SparkSession, kuduContext: KuduContext): Unit = {
    //1、读取ODS表的数据
    import spark.implicits._
    import org.apache.kudu.spark.kudu._
    val odsDF = spark.read.options(options).kudu
    //读取商圈信息
    spark.read.options(areaOptions).kudu.createOrReplaceTempView("t_business_area")
    //2、过滤不符合规范的数据，去重
    //select * from ods where (imei is not null and imei!='') or (mac is not null and mac!='') or
    val filterDF = odsDF.filter(
      """
        |(imei is not null and imei!='') or
        |(mac is not null and mac!='') or
        |(idfa is not null and idfa!='') or
        |(openudid is not null and openudid!='') or
        |(androidid is not null and androidid!='')
      """.stripMargin)

    filterDF.createOrReplaceTempView("t_ods")
    //3、app字典文件广播
    /**
      * 将商圈信息补充到ods中
      */
    spark.udf.register("getGeoHashCode",getGeoHashCode _)

    spark.sql("cache table t_business_area")
    val odsAreaDF = spark.sql(
              """
              select a.*,b.areas
               from t_ods a left join t_business_area b
               on getGeoHashCode(a.longitude,a.latitude) = b.genhash
                """.stripMargin)
    //看执行计划
    //odsAreaDF.explain()

    val appIDName = spark.read.textFile(ConfigUtils.APPID_NAME)
    //将字典文件数据收集到driver端，用于广播
    val appIdNameCollect: Array[(String, String)] = appIDName.map(line => {
      val arr = line.split("##")
      (arr(0), arr(1))
    }).collect()
    val appBc = spark.sparkContext.broadcast(appIdNameCollect.toMap)
    //4、设备字典文件广播
    val device = spark.read.textFile(ConfigUtils.DEVICE_DIC)
    //将字典文件数据收集到driver端，用于广播
    val deviceCollect = device.map(line => {
      val arr = line.split("##")
      (arr(0), arr(1))
    }).collect()
    val deviceBc:Broadcast[Map[String, String]] =  spark.sparkContext.broadcast(deviceCollect.toMap)


    //5、遍历数据生成用户标签
    val currentDayRdd: RDD[(String, (List[String], Map[String, Double]))] = odsAreaDF.rdd.persist().map(row => {
      //1、生成app标签
      val appTag: Map[String, Double] = AppTag.make(row, appBc)

      //2、设备标签
      val deviceTag: Map[String, Double] = DeviceTag.make(row, deviceBc)

      //3、地域标签
      val reionTag = RegionTag.make(row)

      //4、广告位类型
      val adTypeTag = AdTypeTag.make(row)

      //5、关键字
      val keywordTags = KeywordTag.make(row)

      //6、渠道标签
      val channelTag = ChannelTag.make(row)

      //7、年龄标签
      val ageTag = AgeTag.make(row)

      //8、性别标签
      val sexTag = SexTag.make(row)

      //9、商圈标签
      val areaTag = BusinessAreaTag.make(row)
      //10、用户所有标识
      val userIds = getAllUserIds(row)
      //用户唯一标识
      val userId = userIds.head
      //用户所有标签
      val tags = appTag ++ deviceTag ++ reionTag ++ adTypeTag ++ keywordTags ++ channelTag ++ ageTag ++ sexTag ++ areaTag

      (userId, (userIds, tags))
    })
    //用户统一识别
    val graphRdd: RDD[(VertexId, (VertexId, (List[String], Map[String, Double])))] = UserGraph.graph(currentDayRdd)
    //标签聚合
    val currentDayTags: RDD[(VertexId, (List[String], Map[String, Double]))] = TagAgg.agg(graphRdd)
    //当天标签
    val currentTags: RDD[(String, (List[String], Map[String, Double]))] = currentDayTags.map {
      case (aggid, (allUserIds, tags)) =>
        (allUserIds.head, (allUserIds, tags))
    }

    //6、历史数据与当天数据合并
    //读取标签的历史数据
    val historyDF = spark.read.options(historyOptions).kudu

    val historyTag: RDD[(String, (List[String], Map[String, Double]))] = historyDF.rdd.map(row => {
      val userId = row.getAs[String]("userId")

      val allUserIdStr = row.getAs[String]("allUserId")

      val allUserIds: List[String] = allUserIdStr.split(",").toList
      //(BA_永昌,1.0),(BA_永昌,1.0)
      val tagStr = row.getAs[String]("tags")
      //val tags: Map[String, Double]
      //BA_永昌,1.0),(BA_永昌,1.0
      val subStringTag = tagStr.substring(1, tagStr.length - 1)
      //[BA_永昌,1.0 , BA_永昌,1.0]
      val tagArray: Array[String] = subStringTag.split("\\),\\(")

      val tags: Map[String, Double] = tagArray.map(str => {
        val arr = str.split(",")
        val tagName = arr(0)
        val attr = arr(1).toDouble
        (tagName, attr)
      }).toMap

      (userId, (allUserIds, tags))

    })

    //合并历史数据与当天数据
    val allTags = TagMerge.merge(historyTag,currentTags)

    //造一份历史数据
    val result = allTags.map {
      case (userId, (allUserIds, tags)) =>
        (userId, allUserIds.mkString(","), tags.toList.mkString(","))
    }.toDF("userId", "allUserId", "tags")

    //指定tag表的schema
    val schema = result.schema
    //指定分区策略 分区字段 分区数
    val opt = new CreateTableOptions()
    //分区字段
    val columns = Seq[String]("userId")
    import scala.collection.JavaConverters._
    opt.addHashPartitions(columns.asJava,3)
    //指定副本
    opt.setNumReplicas(3)
    //指定主键
    val keys = columns
    KuduUtils.writeToKudu(kuduContext,schema,opt,SINK_TABLE,result,keys)


  }

  /**
    * 获取geohashcode
    * @param longitude
    * @param latitude
    * @return
    */
  def getGeoHashCode(longitude:Float,latitude:Float):String={

    GeoHash.geoHashStringWithCharacterPrecision(latitude.toDouble,longitude.toDouble,8)
  }
  /**
    * 获取用户所有标识
    * @param row
    * @return
    */
  def getAllUserIds(row:Row):List[String]={

    var result = List[String]()

    val imei = row.getAs[String]("imei")

    val mac = row.getAs[String]("mac")

    val idfa = row.getAs[String]("idfa")

    val openudid = row.getAs[String]("openudid")

    val androidid = row.getAs[String]("androidid")

    if(StringUtils.isNotBlank(imei)){
      result = result.:+(imei)
    }

    if(StringUtils.isNotBlank(mac)){
      result = result.:+(mac)
    }

    if(StringUtils.isNotBlank(idfa)){
      result = result.:+(idfa)
    }

    if(StringUtils.isNotBlank(openudid)){
      result = result.:+(openudid)
    }

    if(StringUtils.isNotBlank(androidid)){
      result = result.:+(androidid)
    }
    result
  }
}

```

##### 2.15.5.9.3：ADGraphx代码

```scala
package com.dmp.Graphx

import java.util

import com.dmp.tools.DataUtils
import org.apache.spark.graphx.{Edge, Graph, VertexId, VertexRDD}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.Row

import scala.collection.mutable.ListBuffer

/**
  * Created by angel；
  */
object ADGraphx {

  def graph(rdd : RDD[(String, (List[(String, Int)], List[(String, Double)]))] , odsRDD:RDD[Row]):RDD[(VertexId, (List[(String, Int)], List[(String, Double)]))] = {
    //构建点集合
    val vertices: RDD[(VertexId, (List[(String, Int)], List[(String, Double)]))] = rdd.mapPartitions {
      var listBuffer = new ListBuffer[(Long, (List[(String, Int)], List[(String, Double)]))]()
      line =>
        line.foreach {
          x =>
            //userID和tags
            listBuffer.append((x._1.hashCode.toLong, x._2))
        }
        listBuffer.iterator
    }
    //构建边集合
    val edges: RDD[Edge[Int]] = odsRDD.map {
      line =>
        val userList: util.LinkedList[String] = DataUtils.getTupleID(line)
        val userid = userList.getFirst.toString.hashCode.toLong
        var otherID = new ListBuffer[String]()
        for (index <- 0 until userList.size()) {
          otherID.append(userList.get(index))
        }
        Edge(userid, otherID.toString().hashCode.toLong, 0)
    }
    //    println(vertices.count())

    //构建图
    val graph: Graph[(List[(String, Int)], List[(String, Double)]), Int] = Graph(vertices , edges)
    //让图中的分支连接起来
    val connectVertices: VertexRDD[VertexId] = graph.connectedComponents().vertices
    //将连起来的分支与与点集合做关于userid的join
    val join: RDD[(VertexId, (VertexId, (List[(String, Int)], List[(String, Double)])))] = connectVertices.join(vertices)
    //整理出需要的数据集(最小点ID ， (otherID , tags))
    val data: RDD[(VertexId, (List[(String, Int)], List[(String, Double)]))] = join.map {
      case (userID, (minID, (otherID, tags))) =>
        (minID, (otherID, tags))
    }
    data
  }
}
```

##### 2.15.5.9.4：TagAggreagte代码

```scala
package com.dmp.aggregateTag

import org.apache.spark.graphx.VertexId
import org.apache.spark.rdd.RDD

/**
  * Created by angel；
  */
object TagAggreagte {
  def aggregate(rdd : RDD[(VertexId, (List[(String, Int)], List[(String, Double)]))]): RDD[(VertexId, (List[(String, Int)], List[(String, Double)]))] ={
    //TODO 将同一个顶点的iD的 用户id和标签的聚合操作
    val merge_group: RDD[(VertexId, (List[(String, Int)], List[(String, Double)]))] = rdd.reduceByKey {
      case (before, after) => {
        val uid: List[(String, Int)] = before._1 ++ after._1
        val tagList: List[(String, Double)] = before._2 ++ after._2
        //将同一个用户下的各种标签做加法计算
        val groupTag: Map[String, List[(String, Double)]] = tagList.groupBy(line => line._1)
        val tags: List[(String, Double)] = groupTag.mapValues {
          line =>
            line.foldLeft(0.0)((k, v) => (k + v._2))
        }.toList
        //对uid重复的进行去重
        val distinctUID: List[(String, Int)] = uid.distinct
        (distinctUID, tags)
      }
    }
    merge_group
  }
}
```

##### 2.15.5.9.5：TAGS_Attenuation代码

```scala
package com.dmp.tags


import com.dmp.Graphx.ADGraphx
import com.dmp.aggregateTag.TagAggreagte
import com.dmp.attenu.Attenu
import com.dmp.tools.{DBUtils, DataUtils}
import com.dmp.tools.{ContantsSchemal, GlobalConfUtils}
import org.apache.kudu.spark.kudu._
import org.apache.spark.graphx.{Edge, Graph, VertexId, VertexRDD}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{DataFrame, Row, SQLContext, SaveMode}
/**
  * Created by angel；
  */

object TAGS_Attenuation {
  val coefficient = GlobalConfUtils.coefficient.toDouble
  val KUDU_MASTER = GlobalConfUtils.KUDU_MASTER
  val TO_TABLENAME = GlobalConfUtils.DTP+DataUtils.NowDate()//每天一张表//Kudu使用确定的列类型，而不是类似于NoSQL的“everything is byte”
  /*
  * 读取上一天的数据集，并按照标签衰减计划进行衰减
  * @param row
  * */
  def Merge(lastTableRdd:RDD[Row],todayTableRdd:RDD[Row], ods:RDD[Row] , sqlContext:SQLContext , kuduContext: KuduContext) ={
    //TODO 昨日数据集，并按照衰减规则进行衰减（id ， （全部id ， 标签））
    val last: RDD[(String, (List[(String, Int)], List[(String, Double)]))] = Attenu.attenution(lastTableRdd , coefficient)
    //今天的数据集，做成与历史数据集格式一致（id ， （全部id ， 标签））
    val today: RDD[(String, (List[(String, Int)], List[(String, Double)]))] = operatorTags(todayTableRdd)
    //TODO 合并数据
    val data: RDD[(String, (List[(String,Int)], List[(String, Double)]))] = today.union(last)
    //TODO 统一用户识别
    val graph: RDD[(VertexId, (List[(String,Int)], List[(String, Double)]))] = ADGraphx.graph(data , ods)
    //TODO 将同一个顶点的iD的 用户id和标签的聚合操作
    val merge_group: RDD[(VertexId, (List[(String , Int)], List[(String, Double)]))] = TagAggreagte.aggregate(graph)

    val result = merge_group.map {
      line =>
        val userids = line._2._1.mkString
        val tags = line._2._2.mkString
        (userids , tags)
    }

    //数据落地
    import sqlContext.implicits._
    val sinkdata:DataFrame = result.toDF("userids" , "tags")
    val schema = ContantsSchemal.userTag
    val partitionID = "userids"
    DBUtils.process(kuduContext , sinkdata , TO_TABLENAME , KUDU_MASTER , schema , partitionID)
  }

    //处理今天数据集与历史标签格式一致（id ， （全部id ， 标签））
    def operatorTags(todayTableRdd:RDD[Row]):RDD[(String, (List[(String,Int)], List[(String, Double)]))] = {
      val today_tags : RDD[(String, (List[(String,Int)], List[(String, Double)]))] = todayTableRdd.map {
        row =>
          //"userids" , "tags"
          //(ANDROIDID:DXZDSFURDRRILTPO,0)(MAC:52:54:00:FB:E1:65,0)
          val today_userids = row.getAs[String]("userids")
          // (D00010002,1)(K华为手机,1)(KiPhone,1)(D00020002,1)(APPKK唱响,1)(BA什刹海,景山,1)(D00030003,1)(K华为Mate,1)(K三星,1)(PZ广西百色市,1)(CN123567,1)(LC2,1)(K智能手机,1)(CZ广西百色市,1)
          val today_tags = row.getAs[String]("tags")
          val substring2 = today_tags.substring(1, today_tags.length - 1)
          val arr_tags: Array[String] = substring2.split("\\)\\(")
          var map = Map[String, Double]()
          for (arr <- arr_tags) {
            val weight = arr.split(",")(1).toDouble
            val tagName = arr.split(",")(0)
            map += (tagName -> weight)
          }
          val userids = today_userids.substring(1, today_userids.length - 1)
          val useridarray = userids.split("\\)\\(")//IMEI:21312312312,0)(mac:52525255252,0
        val id = useridarray(0).split(",")(0)
          var uidMap = Map[String, Int]()
          for(arr <- useridarray){
            val id_num = arr.split(",")
            val aid = id_num(0).toString
            val num = id_num(1).toInt
            uidMap += (aid -> num)
          }
          (id, (uidMap.toList, map.toList))

      }
      today_tags
    }
}
```

##### 2.15.5.9.6：数据最终生成效果

```
[angel1:21000] > select * from tag20181102 limit 1;
Query: select * from tag20181102 limit 1
Query submitted at: 2018-11-02 14:42:27 (Coordinator: http://angel1:25000)
Query progress can be monitored at: http://angel1:25000/query_plan?query_id=4344c2c013b45f47:c69c764900000000
+--------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| userids                                                                                                      | tags                                                                                                                                                                                                                                                    |
+--------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| (ANDROIDID:APZWRJWTAHUTJTXD,0)(MAC:52:54:00:37:94:2D,0)(OPENUDID:BLIFGSBGEZFGSMFARIHYDMMSFRCHHZBSTFHYQLDB,0) | (D00020003,2.7664)(D00010002,2.7664)(K汽车用品,2.7664)(APP面包旅行,2.7664)(AGE55,0.0)(K大众汽车,2.7664)(K二手车,2.7664)(CN123522,2.7664)(CZ河南省,2.7664)(D00030001,2.7664)(K大众朗逸,2.7664)(SEX男,0.0)(K德国汽车,2.7664)(PZ河南省,2.7664)(LC2,2.7664) |
+--------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
Fetched 1 row(s) in 0.04s
```


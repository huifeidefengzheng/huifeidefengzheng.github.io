---
typora-root-url: image
---

[TOC]

## 2.4：创建工程

### 2.4.1：创建一个maven项目

![image-20181029155916166](image-20181029155916166.png)

### 2.4.2：创建数据处理模块的工程(ProcessDMP)

![image-20181029160059750](image-20181029160059750.png)



![image-20181029160200874](image-20181029160200874.png)

![image-20181029160217328](image-20181029160217328.png)



## 2.5：使用maven导包：

### 2.5.1：指定Cloudera的maven库

由于部分依赖是需要CDH的包，所以提前指定好Cloudera的maven库

```xml
<repositories>
    <repository>
        <id>cloudera</id>
        <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>
    </repository>
</repositories>
```

### 2.5.2：提前指定好依赖版本

```xml
<properties>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <scala.version>2.11.8</scala.version>
    <scala.v>2.11</scala.v>
    <hadoop.version>2.6.1</hadoop.version>
    <spark.version>2.2.0</spark.version>
    <kudu.version>1.6.0-cdh5.14.0</kudu.version>
    <elasticsearch.verion>6.0.0</elasticsearch.verion>
</properties>
```

### 2.5.3：导入相关依赖

```xml
<dependencies>
 <!-- 导入scala依赖-->
    <dependency>
        <groupId>org.scala-lang</groupId>
        <artifactId>scala-library</artifactId>
        <version>${scala.version}</version>
    </dependency>
    
 <!-- 导入hadoop依赖-->
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>${hadoop.version}</version>
    </dependency>
  <!--导入kudu客户端依赖-->
    <!-- https://mvnrepository.com/artifact/org.apache.kudu/kudu-client -->
    <dependency>
        <groupId>org.apache.kudu</groupId>
        <artifactId>kudu-client</artifactId>
        <version>${kudu.version}</version>
        <scope>test</scope>
    </dependency>

  <!--导入kudu客户端工具类依赖-->
    <!-- https://mvnrepository.com/artifact/org.apache.kudu/kudu-client-tools -->
    <dependency>
        <groupId>org.apache.kudu</groupId>
        <artifactId>kudu-client-tools</artifactId>
        <version>${kudu.version}</version>
    </dependency>

<!--导入kudu整合spark的依赖-->
    <!-- https://mvnrepository.com/artifact/org.apache.kudu/kudu-spark2 -->
    <dependency>
        <groupId>org.apache.kudu</groupId>
        <artifactId>kudu-spark2_${scala.v}</artifactId>
        <version>${kudu.version}</version>
    </dependency>
       
    <!--导入sparkcore依赖-->
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-core_${scala.v}</artifactId>
        <version>${spark.version}</version>
    </dependency>
    
    <!--导入sparksql依赖-->
    <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql -->
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-sql_${scala.v}</artifactId>
        <version>${spark.version}</version>
    </dependency>
    
    <!--导入spark Mlib依赖-->
    <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-mllib -->
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-mllib_${scala.v}</artifactId>
        <version>${spark.version}</version>
    </dependency>

      <!--导入elasticsearch-spark依赖-->  
<!-- https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-20 -->
    <dependency>
        <groupId>org.elasticsearch</groupId>
        <artifactId>elasticsearch-spark-20_${scala.v}</artifactId>
        <version>${elasticsearch.verion}</version>
    </dependency>

     <!--导入spark Graphx依赖-->
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-graphx_${scala.v}</artifactId>
        <version>${spark.version}</version>
    </dependency>

    <!--导入fastjson操作json的依赖-->
    <!-- https://mvnrepository.com/artifact/com.alibaba/fastjson -->
    <dependency>
        <groupId>com.alibaba</groupId>
        <artifactId>fastjson</artifactId>
        <version>1.2.44</version>
    </dependency>
    
   <dependency>
        <groupId>net.sf.json-lib</groupId>
        <artifactId>json-lib</artifactId>
        <version>2.4</version>
        <classifier>jdk15</classifier>
    </dependency>
    
    <!-- 根据ip解析经纬度 -->
    <dependency>
        <groupId>com.maxmind.geoip</groupId>
        <artifactId>geoip-api</artifactId>
        <version>1.3.0</version>
    </dependency>
    <dependency>
        <groupId>com.maxmind.geoip2</groupId>
        <artifactId>geoip2</artifactId>
        <version>2.12.0</version>
    </dependency>
    
    <!--对经纬度进行geohash编码的依赖-->
    <dependency>
        <groupId>ch.hsr</groupId>
        <artifactId>geohash</artifactId>
        <version>1.3.0</version>
    </dependency>

    <!-- 导入impala的依赖 -->
    <dependency>
        <groupId>com.cloudera</groupId>
        <artifactId>ImpalaJDBC41</artifactId>
        <version>2.5.42</version>
    </dependency>
    
    <!-- https://mvnrepository.com/artifact/org.apache.thrift/libfb303 -->
    <!--解决：Caused by: java.lang.ClassNotFoundException: org.apache.thrift.protocol.TPro-->
    <dependency>
        <groupId>org.apache.thrift</groupId>
        <artifactId>libfb303</artifactId>
        <version>0.9.3</version>
        <type>pom</type>
    </dependency>
    
    <!-- https://mvnrepository.com/artifact/org.apache.thrift/libthrift -->
    <!--解决：Caused by: java.lang.ClassNotFoundException: org.apache.thrift.protocol.TPro-->
    <dependency>
        <groupId>org.apache.thrift</groupId>
        <artifactId>libthrift</artifactId>
        <version>0.9.3</version>
        <type>pom</type>
    </dependency>
    
    <!--Caused by: java.lang.ClassNotFoundException: org.apache.hive.service.cli.thrift.TCLIService$Iface-->
    <dependency>
        <groupId>org.apache.hive</groupId>
        <artifactId>hive-jdbc</artifactId>
        <exclusions>
            <exclusion>
                <groupId>org.apache.hive</groupId>
                <artifactId>hive-service-rpc</artifactId>
            </exclusion>
            <exclusion>
                <groupId>org.apache.hive</groupId>
                <artifactId>hive-service</artifactId>
            </exclusion>
        </exclusions>
        <version>1.1.0</version>
    </dependency>
    <!--导入hive的依赖-->
    <dependency>
        <groupId>org.apache.hive</groupId>
        <artifactId>hive-service</artifactId>
        <version>1.1.0</version>
    </dependency>
    
  <!-- 导入加载配置文件的依赖-->
     <dependency>
         <groupId>com.typesafe</groupId>
         <artifactId>config</artifactId>
         <version>1.2.1</version>
     </dependency>
</dependencies>
```

### 2.5.4：将配置文件区分成：开发、生产、测试环境

```XML
<profiles>
    <profile>
        <id>dev</id>
        <activation>
            <!--默认生效的配置组-->
            <activeByDefault>true</activeByDefault>
            <property>
                <name>env</name>
                <value>Dev</value>
            </property>
        </activation>
        <build>
             <!--配置文件路径-->
            <resources>
                <resource>
                    <directory>src/main/resources/dev</directory>
                </resource>
            </resources>
        </build>
    </profile>
    <profile>
        <id>test</id>
        <activation>
            <property>
                <name>env</name>
                <value>Test</value>
            </property>
        </activation>
        <build>
             <!--配置文件路径-->
            <resources>
                <resource>
                    <directory>src/main/resources/test</directory>
                </resource>
            </resources>
        </build>
    </profile>
    <profile>
        <id>prod</id>
        <activation>
            <property>
                <name>env</name>
                <value>Prod</value>
            </property>
        </activation>
        <build>
             <!--配置文件路径-->
            <resources>
                <resource>
                    <directory>src/main/resources/prod</directory>
                </resource>
            </resources>
        </build>
    </profile>
</profiles>
```

然后在工程的resource目录下分别创建：Dev（生产环境）、Prod（开发环境）、Test（测试环境）包

![1566130501219](/../assets/1566130501219.png)



### 2.5.6：开发一个获取配置文件的工具类

对于一个完整的工程来说，如果所有的配置都指定到代码里，就会造成：

1、代码非常的混乱

2、修改配置的时候，需要修改很多的地方

3、数据库相关信息直接暴露在代码里，不安全

综上所述：需要一个配置文件工具类，来专门获取配置文件的内容弄

```scala
package utils

import com.typesafe.config.ConfigFactory

/**
  * 配置文件读取类
  */
object ConfigUtils {
  //加载配置文件
  val conf = ConfigFactory.load()

  val SPARK_SQL_AUTOBROADCASTJOINTHRESHOLD = conf.getString("spark.sql.autoBroadcastJoinThreshold")

  val SPARK_SQL_SHUFFLE_PARTITIONS = conf.getString("spark.sql.shuffle.partitions")

  val SPARK_SHUFFLE_COMPRESS = conf.getString("spark.shuffle.compress")

  val SPARK_SHUFFLE_IO_MAXRETRIES = conf.getString("spark.shuffle.io.maxRetries")

  val SPARK_SHUFFLE_IO_RETRYWAIT = conf.getString("spark.shuffle.io.retryWait")

  val SPARK_BROADCAST_COMPRESS = conf.getString("spark.broadcast.compress")

  val SPARK_SERIALIZER = conf.getString("spark.serializer")

  val SPARK_MEMORY_FRACTION = conf.getString("spark.memory.fraction")

  val SPARK_MEMORY_STORAGEFRACTION = conf.getString("spark.memory.storageFraction")

  val SPARK_DEFAULT_PARALLELISM = conf.getString("spark.default.parallelism")

  val SPARK_LOCALITY_WAIT = conf.getString("spark.locality.wait")

  val SPARK_SPECULATION = conf.getString("spark.speculation.flag")

  val SPARK_SPECULATION_MULTIPLIER = conf.getString("spark.speculation.multiplier")

  //获取数据的路径
  val DATA_PATH = conf.getString("data.path")
  //获取纯真数据库名称
  val IP_FILE = conf.getString("IP_FILE")
  //获取纯真数据库目录
  val INSTALL_DIR = conf.getString("INSTALL_DIR")

  val GeoLiteCity = conf.getString("GeoLiteCity")
  //获取kudu master地址
  val KUDU_MASTER = conf.getString("kudu_master")
  //获取商圈的地址
  val URL = conf.getString("URL")
  //app字段文件路径
  val APPID_NAME = conf.getString("APPID_NAME")
  //设备字典路径
  val DEVICE_DIC = conf.getString("DEVICE_DIC")
  //标签衰减系数
  val ATTENU = conf.getString("attenu")

  val ES_NODES = conf.getString("es.nodes")

  val ES_PORT = conf.getString("es.port")

  val ES_INDEX_AUTO_CREATE = conf.getString("es.index.auto.create")

  val ES_HTTP_TIMEOUT = conf.getString("es.http.timeout")
}
```

### 2.5.7：将相关配置文件放到resource/dev , proc , test中

```scala
#spark参数
#sparksql小表广播出去的大小限制
spark.sql.autoBroadcastJoinThreshold="10485760"
#spark sql shuffle并行度设置
spark.sql.shuffle.partitions="200"
#自动广播超时时间
#spark.sql.broadcastTimeout=""
#是否启动shuffle压缩
spark.shuffle.compress="true"
#shuffle失败重试次数，会自动重试，默认3次
spark.shuffle.io.maxRetries="5"
#shuffle重试的间隔时间
spark.shuffle.io.retryWait="10s"
#广播是否启动压缩
spark.broadcast.compress="true"
#序列化机制
spark.serializer="org.apache.spark.serializer.KryoSerializer"
#存储与执行的内存比例
spark.memory.fraction="0.6"
#存储的内存比例,默认0.5
spark.memory.storageFraction="0.5"
#spark core的shuffle的分区,并行度设置
spark.default.parallelism=200
#数据本地化等待时间,默认3s
spark.locality.wait="3s"
#是否启动推测机制
spark.speculation.flag="true"
#推测机制启动时机
spark.speculation.multiplier="1.5"

#数据路径
data.path="d:/pmt.json"

#纯真数据库名称
IP_FILE="qqwry.dat"

#纯真数据库所处位置
INSTALL_DIR="D:\\DMP\\dmp_class_07\\src\\main\\resources"

GeoLiteCity="D:\\DMP\\dmp_class_07\\src\\main\\resources\\GeoLiteCity.dat"

kudu_master = "hadoop01:7051,hadoop02:7051,hadoop03:7051"
#获取商圈列表URL
URL = "https://restapi.amap.com/v3/geocode/regeo?location=%s&key=df8952997d56956d1a7f6cb438f1230a"
#app字典文件路径
APPID_NAME = "D:\\DMP\\dmp_class_07\\src\\main\\resources\\appID_name"
#设备字典文件路径
DEVICE_DIC = "D:\\DMP\\dmp_class_07\\src\\main\\resources\\devicedic"
#标签衰减系数
attenu = "0.9"
#es集群地址
es.nodes="localhost"
#端口
es.port="9200"
#是否自动创建索引
es.index.auto.create="true"
#超时时间
es.http.timeout="100m"
```

### 2.5.8：创建公特质以及公共工具包

#### 2.5.8.1：创建公共特质

我们需要创建一个公共的特质，定义的ETL操作，只需要实现特质给定的方法即可，主要目的是代码统一化管理

```scala
package `trait`

import org.apache.kudu.spark.kudu.KuduContext
import org.apache.spark.sql.SparkSession

/**
  * 公共介质
  */
trait Process {
  /**
    * 逻辑处理部分，后期不同的操作写不同的逻辑
    */
  def process(spark:SparkSession,kuduContext:KuduContext):Unit
}
```

2.5.8.2：创建公共工具包：

创建一个kudu数据落地的公共工具包，简化每一个业务类的代码量，可读性会更强

```scala
package utils

import org.apache.kudu.Schema
import org.apache.kudu.client.CreateTableOptions
import org.apache.kudu.spark.kudu.KuduContext
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.types.StructType

/**
  * kudu写入帮助类
  */
object KuduUtils {

  def writeToKudu(context:KuduContext, schema:StructType, options:CreateTableOptions,
                  tableName:String, data:DataFrame,keys:Seq[String])={

    //如果表不存在则创建表
    if(!context.tableExists(tableName)){

      context.createTable(tableName,schema,keys,options)
    }

    context.insertRows(data,tableName)
  }
}
```


---
typora-root-url: image
---

[TOC]



#### 2.15.5.3：为什么要进行：spark Graphx统一用户识别

到目前为止已经生成当天用户的标签数据集，如果此时直接进行标签聚合；

那么可能会导致：

对同一受众目标，在标签库中可能会存在多条标签记录！这样会导致后续推荐受众目标出现不准确现象；

举例说明：不进行统一用户识别可能造成的问题：

| 设备型号 | 用户邮箱   | 登录时间 |
| -------- | ---------- | -------- |
| iphone6S | zs@163.com | 上午8点  |
| 华为     | 123@qq.com | 下午3点  |
| iphoneX  | zs@163.com | 晚上10点 |

这样一个用户在一天内产生了3条日志，但是这3条日志的设备型号不一致，所以会被认为这是3个不同用户；

这就导致最后的标签结果有偏差，那么广告主在筛选受众目标的时候会漏掉该用户（比如：广告主要推广广告，但是要求设备型号必须是安卓手机，那么用户在使用苹果手机的时候就接收不到广告，显然是不正确的，并没有达到精准投放）

因此，为了解决上述的问题，我们引入图计算，进行统一用户识别

#### 2.15.5.4：spark Graphx学习

##### 2.15.5.4.1：**图计算的概念简介**

图是用于表示对象之间模型关系的数学结构。图由顶点和连接顶点的边构成。顶点是对象，而边是对象之间的关系。



![image-20181102090849748](image-20181102090849748.png)

有向图是顶点之间的边是有方向的。有向图的例子如 Twitter 上的关注者。用户 Bob 关注了用户 Carol ，而 Carol 并没有关注 Bob。

![image-20181102091009793](image-20181102091009793.png)



以上的简单介绍，就是图，通过点(对象)和边(路径)，构成了不同对象之间的关系

##### 2.15.5.4.2：图计算应用场景

1）：最短路径：

最短路径在社交网络里面，有一个六度空间的理论，表示你和任何一个陌生人之间所间隔的人不会超过五个,也就是说,最多通过五个中间人你就能够认识任何一个陌生人。这也是图算法的一种，也就是说，任何两个人之间的最短路径都是小于等于6。

2）：社群发现：

社群发现用来发现社交网络中三角形的个数（圈子），可以分析出哪些圈子更稳固，关系更紧密，用来衡量社群耦合关系的紧密程度。一个人的社交圈子里面，三角形个数越多，说明他的社交关系越稳固、紧密。像Facebook、Twitter等社交网站，常用到的的社交分析算法就是社群发现。

![image-20181117104014857](image-20181117104014857.png)



参考连接：https://plot.ly/~NaomiZhou/3.embed

3）：推荐算法（ALS）

推荐算法（ALS）ALS是一个矩阵分解算法，比如购物网站要给用户进行商品推荐，就需要知道哪些用户对哪些商品感兴趣，这时，可以通过ALS构建一个矩阵图，在这个矩阵图里，假如被用户购买过的商品是1，没有被用户购买过的是0，这时我们需要计算的就是有哪些0有可能会变成1 

##### 2.15.5.4.3：spark Graphx例子

GraphX 通过弹性分布式属性图扩展了 Sprak RDD。

这种属性图是一种有向多重图，它有多条平行的边。每个边和顶点都有用户定义的属性。平行的边允许相同顶点有多种关系。

通常，在图计算中，基本的数据结构表达就是：G = （V，E，D） V = vertex （顶点或者节点） E = edge （边） D = data （权重）。 

场景：

![image-20181117145607064](image-20181117145607064.png)





| ID   | 姓名   | 年龄 |
| ---- | ------ | ---- |
| 1    | 张三   | 18   |
| 2    | 李四   | 19   |
| 3    | 王五   | 20   |
| 4    | 赵六   | 21   |
| 5    | 韩梅梅 | 22   |
| 6    | 李雷   | 23   |
| 7    | 小明   | 24   |
| 9    | tom    | 25   |
| 10   | jerry  | 26   |
| 11   | ession | 27   |

| UserID | 手机 |
| ------ | ---- |
| 1      | 136  |
| 2      | 136  |
| 3      | 136  |
| 4      | 136  |
| 5      | 136  |
| 4      | 158  |
| 5      | 158  |
| 6      | 158  |
| 7      | 158  |
| 9      | 177  |
| 10     | 177  |
| 11     | 177  |



```scala
import org.apache.spark.graphx.{Edge, Graph, VertexId}
import org.apache.spark.sql.SparkSession

object GraphTest {

  def main(args: Array[String]): Unit = {

    val spark= SparkSession.builder().master("local[*]").appName("test").getOrCreate()
    //1、构建点 (id,（属性）)
    /**
      * 1 张三18
      * 2 李四19
      * 3 王五20
      * 4 赵六21
      * 5 韩梅梅22
      * 6 李雷23
      * 7 小明249tom2510jerry2611ession27
      */
    val vertices = spark.sparkContext.parallelize(Seq[(VertexId, (String, Int))](
      (1, ("张三", 18)),
      (2, ("李四", 19)),
      (3, ("王五", 20)),
      (4, ("赵六", 21)),
      (5, ("韩梅梅", 22)),
      (6, ("李雷", 23)),
      (7, ("小明", 24)),
      (9, ("tom", 25)),
      (10, ("jerry", 26)),
      (11, ("ession", 27))
    ))
    //2、构建边
    val edge = spark.sparkContext.parallelize(Seq[Edge[Int]](
      Edge(1, 133, 0),
      Edge(2, 133, 0),
      Edge(3, 133, 0),
      Edge(4, 133, 0),
      Edge(5, 133, 0),
      Edge(4, 155, 0),
      Edge(5, 155, 0),
      Edge(6, 155, 0),
      Edge(7, 155, 0),
      Edge(9, 188, 0),
      Edge(10, 188, 0),
      Edge(11, 188, 0)
    ))

    //构建图
    val graph = Graph(vertices,edge)
    //查看图中所有点
    //graph.vertices.foreach(println)
    //查看图中所有边
    //graph.edges.foreach(println)
    //println("======点========>"+graph.numVertices)
    //println("======边========>"+graph.numEdges)
    //构建连通图
    val components = graph.connectedComponents()
    //[1,2,3,4,5,6,7]  [9,10,11]
    //components.vertices.map(x=>(x._2,x._1)).groupByKey().foreach
    //连通图点 (id,aggid)
    val vers = components.vertices
    //(id,aggid) join (id,(name,age)) => (id,(aggid,(name,age)))
    //[(1,zhangsan,18),(2,lisi,19)...]  [(9,name,age),(10,name,age),(11,name,age)]
    //(id,com((id,name,age),()))
    vers.join(vertices).map {
      case (id,(aggid,(name,age)))=>
        (aggid,(id,name,age))
    }.groupByKey().collect()

  }
}


```



#### 2.15.5.5：编写：对当天生成的标签进行统一用户识别

##### 2.15.5.5.1：在Merge_tags中添加图计算调用代码

```scala
 //用户统一识别
    val graphRdd: RDD[(VertexId, (VertexId, (List[String], Map[String, Double])))] = UserGraph.graph(currentDayRdd)
```

##### 2.15.5.5.2：编写UserGraph代码

```scala
package graph

import org.apache.spark.graphx.{Edge, Graph, VertexId, VertexRDD}
import org.apache.spark.rdd.RDD

/**
  * 用户统一识别
  */
object UserGraph {

  def graph(currentDayRdd: RDD[(String, (List[String], Map[String, Double]))]):RDD[(VertexId, (VertexId, (List[String], Map[String, Double])))]={

    //1、构建点
    val vertices: RDD[(Long, (List[String],Map[String, Double]))] = currentDayRdd.map {
      case (userid, (allUserIds, tags)) => {
        (userid.hashCode.toLong, (allUserIds,tags))
      }
    }
    //2、构建边
    val edges = currentDayRdd.flatMap {
      case (userid, (allUserIds, tags)) => {
        var result = List[Edge[Int]]()

        allUserIds.foreach(user => {
          result = result.:+(Edge(userid.hashCode.toLong, user.hashCode.toLong, 0))
        })

        result
      }
    }

    //3、构建图
    val graph: Graph[(List[String], Map[String, Double]), Int] = Graph(vertices,edges)
    //4、构建连通图
    val components: Graph[VertexId, Int] = graph.connectedComponents()
    //5、获取用户信息 (id,aggid)
    val componentVertices: VertexRDD[VertexId] = components.vertices
    //(id,aggid)  join (id,(allid,tags))  => (id,(aggid,(allid,tags)))
    val value: RDD[(VertexId, (VertexId, (List[String], Map[String, Double])))] = componentVertices.join(vertices)
    //6、数据返回
    value
  }
}

```









